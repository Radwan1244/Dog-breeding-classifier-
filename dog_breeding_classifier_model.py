# -*- coding: utf-8 -*-
"""Dog breeding classifier model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NhhNcBBidKsNzb0bmakrL0Dq2FxJ2Unc
"""

"""
Dog breeding classifier model
"""
from torchvision import transforms, datasets
from torch.utils.data import DataLoader, random_split
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImagePreprocessingPipeline:
    """
    Comprehensive image preprocessing pipeline for dog breed classification.
    Implements industry-standard preprocessing techniques and data augmentation.
    """

    def configure_image_transforms(self):
        """
        Establishes preprocessing pipelines for both training and validation phases.

        Technical Specifications:
        - Input Image Size: 224x224 pixels (standard for most CNN architectures)
        - Normalization: ImageNet statistics
        - Augmentation: Moderate to prevent overfitting while maintaining feature integrity

        Returns:
            dict: Transform configurations for training and validation
        """
        # Training pipeline with augmentation
        training_pipeline = transforms.Compose([
            transforms.RandomResizedCrop(
                size=224,
                scale=(0.8, 1.0)
            ),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(
                degrees=15,
                fill=(255,)
            ),
            transforms.ColorJitter(
                brightness=0.2,
                contrast=0.2,
                saturation=0.1
            ),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

        # Validation pipeline (deterministic)
        validation_pipeline = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

        return {
            'train': training_pipeline,
            'validation': validation_pipeline
        }

    def prepare_data_loaders(self, data_path, train_split=0.8):
        """
        Constructs data loading pipeline with proper splitting and augmentation.

        Parameters:
            data_path (str): Directory containing breed-specific subdirectories
            train_split (float): Training data proportion (default: 0.8)

        Returns:
            tuple: (train_loader, val_loader)
        """
        transforms_dict = self.configure_image_transforms()

        logger.info(f"Loading dataset from {data_path}")

        # Load dataset with training transforms initially
        dataset = datasets.ImageFolder(
            root=data_path,
            transform=transforms_dict['train']
        )

        # Calculate split sizes
        dataset_size = len(dataset)
        train_count = int(train_split * dataset_size)
        val_count = dataset_size - train_count

        logger.info(f"Dataset split: {train_count} training, {val_count} validation samples")

        # Create dataset splits
        train_data, val_data = random_split(
            dataset,
            [train_count, val_count],
            generator=torch.Generator().manual_seed(42)
        )

        # Apply appropriate transforms to validation split
        val_data.dataset.transform = transforms_dict['validation']

        # Configure data loaders
        train_loader = DataLoader(
            train_data,
            batch_size=32,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )

        val_loader = DataLoader(
            val_data,
            batch_size=32,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )

        return train_loader, val_loader